{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial Contribution Maps and B-cos Explanations\n",
    "\n",
    "def to_numpy_img(tensor):\n",
    "    \"\"\"Convert tensor to numpy array for matplotlib display\"\"\"\n",
    "    if len(tensor.shape) == 4:\n",
    "        tensor = tensor.squeeze(0)\n",
    "    if len(tensor.shape) == 3:\n",
    "        if tensor.shape[0] <= 3:  # CHW format\n",
    "            tensor = tensor.permute(1, 2, 0)\n",
    "    # Handle 6-channel input by taking RGB channels\n",
    "    if tensor.shape[-1] == 6:\n",
    "        tensor = tensor[:, :, :3]\n",
    "    # Ensure valid range for display\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    return tensor.detach().cpu().numpy()\n",
    "\n",
    "def extract_prototype_patches(model, dataloader, prototype_idx, num_patches=9):\n",
    "    \"\"\"Extract top-activating patches for a prototype\"\"\"\n",
    "    model.eval()\n",
    "    patches = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            out = model(imgs)\n",
    "            sim_maps = out[\"sim_maps\"]\n",
    "            \n",
    "            # Get similarities for specific prototype\n",
    "            proto_sims = sim_maps[:, prototype_idx]  # (B, H, W)\n",
    "            \n",
    "            for i in range(proto_sims.shape[0]):\n",
    "                if len(proto_sims.shape) == 3:\n",
    "                    max_sim = proto_sims[i].max().item()\n",
    "                else:\n",
    "                    max_sim = proto_sims[i].item() if proto_sims[i].numel() == 1 else proto_sims[i].max().item()\n",
    "                    \n",
    "                if max_sim > 0.1:  # Threshold\n",
    "                    patches.append({\n",
    "                        'image': imgs[i],\n",
    "                        'similarity': max_sim,\n",
    "                        'sim_map': proto_sims[i] if len(proto_sims.shape) == 3 else proto_sims[i].unsqueeze(0)\n",
    "                    })\n",
    "                    \n",
    "            if len(patches) >= num_patches:\n",
    "                break\n",
    "    \n",
    "    # Sort by similarity and return top patches\n",
    "    patches.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    return patches[:num_patches]\n",
    "\n",
    "def visualize_prototype_analysis(model, cfg, test_loader, proto_idx):\n",
    "    \"\"\"Analyze and visualize a specific prototype\"\"\"\n",
    "    print(f\"Analyzing prototype {proto_idx}...\")\n",
    "    patches = extract_prototype_patches(model, test_loader, proto_idx, 6)\n",
    "    \n",
    "    if patches:\n",
    "        print(f\"Found {len(patches)} activating patches\")\n",
    "        print(f\"Top similarity: {patches[0]['similarity']:.3f}\")\n",
    "        print(f\"Prototype class: {proto_idx // cfg.num_prototypes_per_class}\")\n",
    "    else:\n",
    "        print(\"No significant activations found\")\n",
    "\n",
    "class BcosSpatialContributionAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzer for B-cos spatial contributions integrated with PIP-Net\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device='cuda'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Store intermediate activations\n",
    "        self.activations = {}\n",
    "        self.gradients = {}\n",
    "        self.hooks = []\n",
    "        \n",
    "        # Register hooks for B-cos explanations\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register forward and backward hooks for B-cos layers\"\"\"\n",
    "        def save_activation(name):\n",
    "            def hook(module, input, output):\n",
    "                self.activations[name] = output.detach()\n",
    "            return hook\n",
    "        \n",
    "        def save_gradient(name):\n",
    "            def hook(module, grad_input, grad_output):\n",
    "                if grad_output[0] is not None:\n",
    "                    self.gradients[name] = grad_output[0].detach()\n",
    "            return hook\n",
    "        \n",
    "        # Register hooks for B-cos layers\n",
    "        for name, module in self.model.named_modules():\n",
    "            if hasattr(module, 'explanation_mode'):  # B-cos layers\n",
    "                handle_forward = module.register_forward_hook(save_activation(name))\n",
    "                handle_backward = module.register_backward_hook(save_gradient(name))\n",
    "                self.hooks.extend([handle_forward, handle_backward])\n",
    "    \n",
    "    def cleanup_hooks(self):\n",
    "        \"\"\"Remove all registered hooks\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "    \n",
    "    def get_bcos_contributions(self, image_tensor, target_class=None):\n",
    "        \"\"\"\n",
    "        Get B-cos spatial contribution maps using gradients\n",
    "        \n",
    "        Args:\n",
    "            image_tensor: Input tensor (1, 3, H, W)\n",
    "            target_class: Target class for explanation (if None, use predicted class)\n",
    "        \n",
    "        Returns:\n",
    "            contribution_maps: Dictionary of contribution maps from B-cos layers\n",
    "            predicted_class: Predicted class index\n",
    "            class_score: Confidence score for predicted class\n",
    "        \"\"\"\n",
    "        # Clear previous activations\n",
    "        self.activations.clear()\n",
    "        self.gradients.clear()\n",
    "        \n",
    "        # Enable explanation mode for B-cos layers\n",
    "        self.model.explanation_mode(detach=True)\n",
    "        \n",
    "        # Enable gradients\n",
    "        image_tensor.requires_grad_(True)\n",
    "        \n",
    "        # Forward pass\n",
    "        out = self.model(image_tensor.to(self.device))\n",
    "        logits = out[\"logits\"]\n",
    "        \n",
    "        # Get target class\n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(logits, dim=1)\n",
    "        else:\n",
    "            target_class = torch.tensor([target_class]).to(self.device)\n",
    "        \n",
    "        predicted_class = target_class.item()\n",
    "        class_score = torch.softmax(logits, dim=1)[0, predicted_class].item()\n",
    "        \n",
    "        # Backward pass\n",
    "        class_output = logits[0, target_class]\n",
    "        class_output.backward(retain_graph=True)\n",
    "        \n",
    "        # Generate contribution maps\n",
    "        contribution_maps = {}\n",
    "        \n",
    "        for name, activation in self.activations.items():\n",
    "            if name in self.gradients:\n",
    "                gradient = self.gradients[name]\n",
    "                \n",
    "                # Compute contribution as activation * gradient\n",
    "                contribution = activation * gradient\n",
    "                \n",
    "                # Sum over channels to get spatial contribution\n",
    "                spatial_contribution = contribution.sum(dim=1, keepdim=True)  # (1, 1, H, W)\n",
    "                \n",
    "                contribution_maps[name] = spatial_contribution.squeeze(0).squeeze(0).cpu().numpy()\n",
    "        \n",
    "        return contribution_maps, predicted_class, class_score\n",
    "    \n",
    "    def get_input_gradient_contribution(self, image_tensor, target_class=None):\n",
    "        \"\"\"\n",
    "        Get input-level gradient contribution map (Input x Gradient)\n",
    "        \"\"\"\n",
    "        # Clear previous activations\n",
    "        self.activations.clear()\n",
    "        self.gradients.clear()\n",
    "        \n",
    "        # Enable explanation mode\n",
    "        self.model.explanation_mode(detach=True)\n",
    "        \n",
    "        # Enable gradients for input\n",
    "        image_tensor.requires_grad_(True)\n",
    "        \n",
    "        # Forward pass\n",
    "        out = self.model(image_tensor.to(self.device))\n",
    "        logits = out[\"logits\"]\n",
    "        encoded_input = out[\"encoded_input\"]  # 6-channel encoded input\n",
    "        \n",
    "        # Get target class\n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(logits, dim=1)\n",
    "        else:\n",
    "            target_class = torch.tensor([target_class]).to(self.device)\n",
    "        \n",
    "        predicted_class = target_class.item()\n",
    "        class_score = torch.softmax(logits, dim=1)[0, predicted_class].item()\n",
    "        \n",
    "        # Backward pass to get input gradients\n",
    "        class_output = logits[0, target_class]\n",
    "        class_output.backward(retain_graph=True)\n",
    "        \n",
    "        # Get gradients w.r.t. the 6-channel encoded input\n",
    "        input_grad = image_tensor.grad  # This will be (1, 3, H, W) for original input\n",
    "        \n",
    "        # We need gradients w.r.t. encoded input for B-cos explanation\n",
    "        encoded_input.requires_grad_(True)\n",
    "        encoded_input.retain_grad()\n",
    "        \n",
    "        # Re-run forward with encoded input requiring gradients\n",
    "        image_tensor.grad = None  # Clear previous gradients\n",
    "        out2 = self.model.features(encoded_input)  # Skip encoder, use encoded input directly\n",
    "        sims_max, locs, sim_maps = self.model.prototype_layer(out2)\n",
    "        scores = self.model.classifier(sims_max)\n",
    "        logits2 = torch.log(scores.pow(2) + 1.0)\n",
    "        \n",
    "        class_output2 = logits2[0, target_class]\n",
    "        class_output2.backward()\n",
    "        \n",
    "        # Get gradients w.r.t. encoded input\n",
    "        encoded_grad = encoded_input.grad  # (1, 6, H, W)\n",
    "        \n",
    "        if BCOS_UTILS_AVAILABLE:\n",
    "            # Use official B-cos grad_to_img function\n",
    "            bcos_explanation = grad_to_img(encoded_input[0], encoded_grad[0])\n",
    "        else:\n",
    "            # Fallback implementation\n",
    "            bcos_explanation = self._grad_to_img_fallback(encoded_input[0], encoded_grad[0])\n",
    "        \n",
    "        # Input * Gradient contribution\n",
    "        input_contrib = (image_tensor[0] * input_grad[0]).sum(0).cpu().numpy()  # Sum over RGB channels\n",
    "        \n",
    "        return {\n",
    "            'input_contribution': input_contrib,\n",
    "            'bcos_explanation': bcos_explanation,\n",
    "            'predicted_class': predicted_class,\n",
    "            'class_score': class_score,\n",
    "            'input_gradient': input_grad[0].cpu().numpy(),\n",
    "            'encoded_gradient': encoded_grad[0].cpu().numpy()\n",
    "        }\n",
    "    \n",
    "    def _grad_to_img_fallback(self, img_6ch, linear_mapping, smooth=3, alpha_percentile=99.5):\n",
    "        \"\"\"Fallback B-cos grad_to_img implementation\"\"\"\n",
    "        # Ensure tensors are on CPU and detached\n",
    "        if hasattr(img_6ch, 'detach'):\n",
    "            img_6ch = img_6ch.detach().cpu()\n",
    "        if hasattr(linear_mapping, 'detach'):\n",
    "            linear_mapping = linear_mapping.detach().cpu()\n",
    "        \n",
    "        # Compute contributions\n",
    "        contribs = (img_6ch * linear_mapping).sum(0, keepdim=True)[0]\n",
    "        \n",
    "        # Normalize gradient\n",
    "        rgb_grad = (linear_mapping / (linear_mapping.abs().max(0, keepdim=True)[0] + 1e-12))\n",
    "        rgb_grad = rgb_grad.clamp(0)\n",
    "        rgb_grad = rgb_grad[:3] / (rgb_grad[:3] + rgb_grad[3:] + 1e-12)\n",
    "        \n",
    "        # Set alpha\n",
    "        alpha = linear_mapping.norm(p=2, dim=0, keepdim=True)\n",
    "        alpha = torch.where(contribs[None] < 0, torch.zeros_like(alpha) + 1e-12, alpha)\n",
    "        \n",
    "        if smooth > 1:\n",
    "            alpha = F.avg_pool2d(alpha, smooth, stride=1, padding=(smooth-1)//2)\n",
    "        \n",
    "        alpha = alpha.numpy()\n",
    "        alpha = (alpha / np.percentile(alpha, alpha_percentile)).clip(0, 1)\n",
    "        rgb_grad = rgb_grad.numpy()\n",
    "        \n",
    "        rgba_grad = np.concatenate([rgb_grad, alpha], axis=0)\n",
    "        return rgba_grad.transpose((1, 2, 0))\n",
    "    \n",
    "    def visualize_comprehensive_explanation(self, image_tensor, target_class=None, \n",
    "                                          figsize=(20, 12), class_names=None):\n",
    "        \"\"\"\n",
    "        Create comprehensive visualization with:\n",
    "        1. Input image\n",
    "        2. Prototype activations\n",
    "        3. B-cos spatial contributions\n",
    "        4. Input-level explanations\n",
    "        \"\"\"\n",
    "        \n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        gs = GridSpec(3, 4, figure=fig, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Original image\n",
    "        ax_orig = fig.add_subplot(gs[0, 0])\n",
    "        orig_img = to_numpy_img(image_tensor)\n",
    "        ax_orig.imshow(orig_img)\n",
    "        ax_orig.set_title('Original Image', fontweight='bold')\n",
    "        ax_orig.axis('off')\n",
    "        \n",
    "        # Get B-cos contributions\n",
    "        contributions, pred_class, class_score = self.get_bcos_contributions(image_tensor, target_class)\n",
    "        \n",
    "        # Get input-level explanation\n",
    "        input_explanation = self.get_input_gradient_contribution(image_tensor, target_class)\n",
    "        \n",
    "        # Prediction info\n",
    "        ax_pred = fig.add_subplot(gs[0, 1])\n",
    "        class_name = class_names[pred_class] if class_names and pred_class < len(class_names) else f\"Class {pred_class}\"\n",
    "        ax_pred.text(0.1, 0.7, f'Prediction:', fontsize=12, fontweight='bold')\n",
    "        ax_pred.text(0.1, 0.5, class_name, fontsize=14, color='red')\n",
    "        ax_pred.text(0.1, 0.3, f'Confidence: {class_score:.3f}', fontsize=12)\n",
    "        ax_pred.axis('off')\n",
    "        \n",
    "        # Input * Gradient contribution\n",
    "        ax_input = fig.add_subplot(gs[0, 2])\n",
    "        input_contrib = input_explanation['input_contribution']\n",
    "        ax_input.imshow(orig_img, alpha=0.7)\n",
    "        vmax = np.abs(input_contrib).max()\n",
    "        im1 = ax_input.imshow(input_contrib, cmap='RdBu_r', alpha=0.8, vmin=-vmax, vmax=vmax)\n",
    "        ax_input.set_title('Input × Gradient')\n",
    "        ax_input.axis('off')\n",
    "        plt.colorbar(im1, ax=ax_input, fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # B-cos explanation (RGBA)\n",
    "        ax_bcos = fig.add_subplot(gs[0, 3])\n",
    "        bcos_rgba = input_explanation['bcos_explanation']\n",
    "        ax_bcos.imshow(bcos_rgba)\n",
    "        ax_bcos.set_title('B-cos Explanation (RGBA)')\n",
    "        ax_bcos.axis('off')\n",
    "        \n",
    "        # Layer-wise B-cos contributions\n",
    "        layer_names = list(contributions.keys())[:6]  # Show top 6 layers\n",
    "        for idx, layer_name in enumerate(layer_names):\n",
    "            row = 1 + idx // 3\n",
    "            col = idx % 3\n",
    "            \n",
    "            if row < 3:  # Ensure we don't exceed grid\n",
    "                ax = fig.add_subplot(gs[row, col])\n",
    "                \n",
    "                contrib_map = contributions[layer_name]\n",
    "                # Resize to match image\n",
    "                contrib_resized = cv2.resize(contrib_map, (orig_img.shape[1], orig_img.shape[0]))\n",
    "                \n",
    "                ax.imshow(orig_img, alpha=0.6)\n",
    "                vmax = np.abs(contrib_resized).max()\n",
    "                im = ax.imshow(contrib_resized, cmap='RdBu_r', alpha=0.8, vmin=-vmax, vmax=vmax)\n",
    "                \n",
    "                display_name = layer_name.split('.')[-1] if '.' in layer_name else layer_name\n",
    "                ax.set_title(f'{display_name}', fontsize=10)\n",
    "                ax.axis('off')\n",
    "                plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        \n",
    "        plt.suptitle('B-cos PIP-Net: Comprehensive Spatial Contribution Analysis', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        \n",
    "        return {\n",
    "            'contributions': contributions,\n",
    "            'input_explanation': input_explanation,\n",
    "            'predicted_class': pred_class,\n",
    "            'class_score': class_score\n",
    "        }\n",
    "\n",
    "# Usage example function\n",
    "def analyze_single_image_comprehensive(model, image_tensor, cfg, class_names=None):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of a single image showing all interpretability aspects\n",
    "    \"\"\"\n",
    "    print(\"🔍 Comprehensive B-cos PIP-Net Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = BcosSpatialContributionAnalyzer(model, device)\n",
    "    \n",
    "    try:\n",
    "        # Run comprehensive visualization\n",
    "        results = analyzer.visualize_comprehensive_explanation(\n",
    "            image_tensor, \n",
    "            figsize=(20, 15),\n",
    "            class_names=class_names\n",
    "        )\n",
    "        \n",
    "        # Print summary\n",
    "        pred_class = results['predicted_class']\n",
    "        class_score = results['class_score']\n",
    "        \n",
    "        print(f\"Prediction: Class {pred_class} (confidence: {class_score:.3f})\")\n",
    "        print(f\"Number of B-cos layers analyzed: {len(results['contributions'])}\")\n",
    "        print(f\"B-cos explanation shape: {results['input_explanation']['bcos_explanation'].shape}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    finally:\n",
    "        # Always cleanup hooks\n",
    "        analyzer.cleanup_hooks()\n",
    "\n",
    "print(\"✅ Spatial contribution maps and B-cos explanations ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-76liMNAhTn"
   },
   "source": [
    "B-cos PiP-Net (improved, no ReLU/BN/MaxPool): LA+LT pretraining + supervised.\n",
    "Downsampling ONLY via strided B-cos convs. Explanations stay compact via. Non-negative scoring-sheet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOvzcfWqBJNa"
   },
   "outputs": [],
   "source": [
    "import os, time, math, json, random\n",
    "from datetime import timedelta\n",
    "from collections import OrderedDict\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Add PIPNet and visualization modules to path\n",
    "import sys\n",
    "sys.path.append('./PIPNet')\n",
    "sys.path.append('./src')\n",
    "from pipnet.pipnet import NonNegLinear\n",
    "\n",
    "# B-cos interpretability utilities\n",
    "sys.path.append('./B-cos')\n",
    "try:\n",
    "    from interpretability.utils import grad_to_img, plot_contribution_map, explanation_mode\n",
    "    from project_utils import to_numpy\n",
    "    BCOS_UTILS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"B-cos interpretability utils not available, using fallback implementations\")\n",
    "    BCOS_UTILS_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMwOHWEaBLz8"
   },
   "source": [
    "#Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vbl1iBGHBO66",
    "outputId": "76a04bd2-f300-4553-a268-4a435b1d3899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True | Device: cuda\n",
      "NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "seed_everything(42)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()} | Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff-jvlhOBTWq"
   },
   "source": [
    "#B-cos modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f26ELHL2BVjm"
   },
   "outputs": [],
   "source": [
    "# Import proper B-cos modules\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('./B-cos')\n",
    "\n",
    "from modules.bcosconv2d import BcosConv2d as OriginalBcosConv2d\n",
    "from data.data_transforms import AddInverse\n",
    "\n",
    "class BcosInputEncoder(nn.Module):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.max() > 1.1: x = x / 255.0\n",
    "        x = x.clamp(0.0, 1.0)\n",
    "        return torch.cat([x, 1.0 - x], dim=1)  # [r,g,b,1-r,1-g,1-b]\n",
    "\n",
    "class BcosConv2d(OriginalBcosConv2d):\n",
    "    \"\"\"\n",
    "    Proper B-cos conv using original implementation from /B-cos\n",
    "    Wrapper to match our interface while using correct B-cos math\n",
    "    \"\"\"\n",
    "    def __init__(self, inc, outc, k=3, s=1, p=1, bias=False, B=2, max_out=1, **kwargs):\n",
    "        # Convert our interface to original B-cos interface\n",
    "        super().__init__(\n",
    "            inc=inc, \n",
    "            outc=outc, \n",
    "            kernel_size=k, \n",
    "            stride=s, \n",
    "            padding=p, \n",
    "            b=B,\n",
    "            max_out=max_out,\n",
    "            **kwargs\n",
    "        )\n",
    "        # Note: Original B-cos has built-in max_out, bias is always False\n",
    "\n",
    "class BcosLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    B-cos linear layer following the same principles as BcosConv2d\n",
    "    \"\"\"\n",
    "    def __init__(self, in_f, out_f, bias=True, B=2):\n",
    "        super().__init__()\n",
    "        from modules.bcosconv2d import NormedConv2d\n",
    "        self.weight = nn.Parameter(torch.randn(out_f, in_f) * 0.1)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_f)) if bias else None\n",
    "        self.B = float(B)\n",
    "        self.detach = False\n",
    "        \n",
    "        # Proper scaling following B-cos principles\n",
    "        self.scale = (np.sqrt(in_f)) / 100  # Similar to BcosConv2d scale calculation\n",
    "    \n",
    "    def explanation_mode(self, detach=True):\n",
    "        \"\"\"Enable explanation mode like in BcosConv2d\"\"\"\n",
    "        self.detach = detach\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Proper B-cos linear computation following the same pattern as conv\n",
    "        \n",
    "        # Normalize weights\n",
    "        w_hat = self.weight / (self.weight.norm(p=2, dim=1, keepdim=True) + 1e-8)\n",
    "        z_lin = F.linear(x, w_hat, self.bias)\n",
    "        \n",
    "        if self.B == 1:\n",
    "            return z_lin / self.scale\n",
    "        \n",
    "        # Compute input norm\n",
    "        x_norm = x.norm(p=2, dim=1, keepdim=True) + 1e-6\n",
    "        \n",
    "        # Cosine computation\n",
    "        cos_sim = z_lin / x_norm\n",
    "        abs_cos = cos_sim.abs() + 1e-6\n",
    "        \n",
    "        # Apply detaching for explanation mode\n",
    "        if self.detach:\n",
    "            abs_cos = abs_cos.detach()\n",
    "        \n",
    "        # B-cos transformation: multiply by |cos|^(B-1) to get cos^B effect\n",
    "        out = z_lin * abs_cos.pow(self.B - 1)\n",
    "        return out / self.scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_NS9So7BbkY"
   },
   "source": [
    "#Prototype layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "xNPT2U_9Bdxn"
   },
   "outputs": [],
   "source": [
    "class PrototypeLayer(nn.Module):\n",
    "    def __init__(self, num_prototypes: int, prototype_shape: Tuple[int,int,int], eps=1e-6):\n",
    "        super().__init__()\n",
    "        C,kH,kW = prototype_shape\n",
    "        self.prototypes = nn.Parameter(torch.randn(num_prototypes, C, kH, kW) * 0.1)\n",
    "        self.eps = eps\n",
    "    @property\n",
    "    def P(self): return self.prototypes.size(0)\n",
    "    def _norm_protos(self):\n",
    "        p = self.prototypes.view(self.P, -1)\n",
    "        p = p / (p.norm(p=2, dim=1, keepdim=True) + self.eps)\n",
    "        return p.view_as(self.prototypes)\n",
    "    def forward(self, x):\n",
    "        x_norm = F.normalize(x, p=2, dim=1, eps=self.eps)\n",
    "        p_norm = self._norm_protos()\n",
    "        sim_maps = F.conv2d(x_norm, p_norm, bias=None, stride=1, padding=0)  # (B,P,Hout,Wout)\n",
    "        Bsz,P,H,W = sim_maps.shape\n",
    "        maxv, idx = sim_maps.view(Bsz,P,-1).max(dim=2)\n",
    "        row, col = (idx // W).float(), (idx % W).float()\n",
    "        loc = torch.stack([row, col], dim=2)\n",
    "        return maxv, loc, sim_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBOtGlOtBf6Z"
   },
   "source": [
    "#Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rL24qMWBBha6"
   },
   "outputs": [],
   "source": [
    "class BcosPiPNet(nn.Module):\n",
    "    \"\"\"\n",
    "    No ReLU/BN/MaxPool. Downsampling via strided B-cos convs only.\n",
    "    Uses proper B-cos implementation with built-in max_out.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, num_prototypes_per_class=15,\n",
    "                 prototype_shape=(128,1,1), base_channels=64, B=2,\n",
    "                 patchify_stem=True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_prototypes_per_class = num_prototypes_per_class\n",
    "        self.num_prototypes = num_classes * num_prototypes_per_class\n",
    "        self.prototype_shape = prototype_shape\n",
    "\n",
    "        self.input_encoder = BcosInputEncoder()\n",
    "        in_ch = 6\n",
    "        Cproto = prototype_shape[0]\n",
    "\n",
    "        feats = []\n",
    "        if patchify_stem:\n",
    "            feats += [(\"stem\", BcosConv2d(in_ch, base_channels, k=4, s=4, p=0, B=B))]   # /4\n",
    "        else:\n",
    "            feats += [(\"conv0\", BcosConv2d(in_ch, base_channels, k=3, s=1, p=1, B=B))]\n",
    "\n",
    "        # Use proper B-cos with built-in max_out (max_out=2 means 2x output channels, then max)\n",
    "        feats += [\n",
    "            (\"conv1\", BcosConv2d(base_channels, base_channels, k=3, s=1, p=1, B=B, max_out=2)),  # Built-in MaxOut\n",
    "            (\"down1\", BcosConv2d(base_channels, base_channels*2, k=3, s=2, p=1, B=B, max_out=2)),  # /2, built-in MaxOut\n",
    "            \n",
    "            (\"conv2\", BcosConv2d(base_channels, base_channels*2, k=3, s=1, p=1, B=B, max_out=2)),  # Built-in MaxOut\n",
    "            (\"down2\", BcosConv2d(base_channels, base_channels*2, k=3, s=2, p=1, B=B, max_out=2)),  # /2, built-in MaxOut\n",
    "            \n",
    "            (\"conv3\", BcosConv2d(base_channels, Cproto, k=3, s=1, p=1, B=B, max_out=2)),  # Built-in MaxOut\n",
    "            (\"conv4\", BcosConv2d(Cproto//2, Cproto, k=3, s=1, p=1, B=B)),  # No MaxOut for final layer\n",
    "        ]\n",
    "\n",
    "        self.features = nn.Sequential(OrderedDict(feats))\n",
    "\n",
    "        self.prototype_layer = PrototypeLayer(self.num_prototypes, prototype_shape)\n",
    "        self.classifier = NonNegLinear(self.num_prototypes, num_classes)\n",
    "        self._init_classifier_bias()\n",
    "\n",
    "    def _init_classifier_bias(self):\n",
    "        with torch.no_grad():\n",
    "            W = torch.zeros(self.num_prototypes, self.num_classes)\n",
    "            for c in range(self.num_classes):\n",
    "                s, e = c*self.num_prototypes_per_class, (c+1)*self.num_prototypes_per_class\n",
    "                W[s:e, c] = 1.0\n",
    "            W += 0.05 * torch.randn_like(W)\n",
    "            # inverse softplus approx to seed non-negative weights\n",
    "            self.classifier._w.copy_(torch.log(torch.expm1(W.clamp_min(1e-4))))\n",
    "\n",
    "    def explanation_mode(self, detach=True):\n",
    "        \"\"\"Enable explanation mode for all B-cos layers\"\"\"\n",
    "        for module in self.modules():\n",
    "            if hasattr(module, 'explanation_mode'):\n",
    "                module.explanation_mode(detach)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.input_encoder(x)\n",
    "        feats = self.features(enc)\n",
    "        sims_max, locs, sim_maps = self.prototype_layer(feats)       # sims_max: (B,P), sim_maps: (B,P,H,W)\n",
    "\n",
    "        scores = self.classifier(sims_max)                           # (B,C)\n",
    "        logits = torch.log(scores.pow(2) + 1.0)                      # training logits (keep abstention & compactness)\n",
    "        return {\"logits\": logits, \"similarities\": sims_max, \"sim_maps\": sim_maps,\n",
    "                \"locations\": locs, \"features\": feats, \"encoded_input\": enc}\n",
    "\n",
    "    def get_prototype_activations(self, x, return_features=False):\n",
    "        \"\"\"\n",
    "        Get prototype activations for contrastive learning.\n",
    "        Returns similarity maps and pooled similarities.\n",
    "        \"\"\"\n",
    "        enc = self.input_encoder(x)\n",
    "        feats = self.features(enc)\n",
    "        sims_max, locs, sim_maps = self.prototype_layer(feats)\n",
    "        \n",
    "        if return_features:\n",
    "            return sim_maps, sims_max, feats\n",
    "        return sim_maps, sims_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVYl9ioEBmjN"
   },
   "source": [
    "#Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVN-dgKYBnzA"
   },
   "outputs": [],
   "source": "class ContrastiveLoss(nn.Module):\n    \"\"\"\n    Contrastive learning loss combining Alignment Loss (LA) and Tanh Loss (LT)\n    \"\"\"\n    def __init__(self, align_weight=1.0, tanh_weight=1.0):\n        super().__init__()\n        self.align_weight = align_weight\n        self.tanh_weight = tanh_weight\n    \n    def forward(self, sim_maps1, sim_maps2):\n        \"\"\"\n        Args:\n            sim_maps1, sim_maps2: (B,P,H,W) similarity maps from two augmented views\n        \"\"\"\n        # Convert similarity maps to prototype presence\n        z1, p1 = proto_presence_from_sim_maps(sim_maps1)\n        z2, p2 = proto_presence_from_sim_maps(sim_maps2)\n        \n        # Alignment loss: encourage consistency between views\n        LA = loss_LA(z1, z2)\n        \n        # Tanh diversity loss: encourage diverse prototype usage\n        LT = loss_LT(torch.cat([p1, p2], dim=0))\n        \n        total_loss = self.align_weight * LA + self.tanh_weight * LT\n        \n        return {\n            'total_loss': total_loss,\n            'align_loss': LA,\n            'tanh_loss': LT\n        }\n\nclass BcosPiPNetLoss(nn.Module):\n    \"\"\"\n    B-cos compatible loss with BCE, temperature scaling, and logit bias\n    Following official B-cos implementation patterns\n    \"\"\"\n    def __init__(self, num_classes, class_weight=1.0, cluster_weight=0.6, separation_weight=0.05, l1_weight=2e-4, \n                 use_bce=True, logit_bias=None, logit_temperature=1.0):\n        super().__init__()\n        self.num_classes = num_classes\n        self.class_weight, self.cluster_weight = class_weight, cluster_weight\n        self.separation_weight, self.l1_weight = separation_weight, l1_weight\n        self.use_bce = use_bce\n        self.logit_bias = logit_bias if logit_bias is not None else np.log(0.1/0.9)  # B-cos default\n        self.logit_temperature = logit_temperature\n        \n        # Loss functions following B-cos implementation\n        if use_bce:\n            self.classification_loss = nn.BCEWithLogitsLoss(reduction='mean')\n        else:\n            self.classification_loss = nn.CrossEntropyLoss(label_smoothing=0.05)\n\n    def forward(self, out, targets, model):\n        logits, sims = out[\"logits\"], out[\"similarities\"]\n        \n        # Apply B-cos temperature scaling and bias (following B-cos FinalLayer)\n        processed_logits = logits / self.logit_temperature + self.logit_bias\n        \n        # Classification loss (B-cos style)\n        if self.use_bce:\n            # Convert targets to one-hot for BCE (B-cos requirement)\n            targets_one_hot = F.one_hot(targets, num_classes=self.num_classes).float()\n            classification_loss = self.classification_loss(processed_logits, targets_one_hot)\n        else:\n            classification_loss = self.classification_loss(processed_logits, targets)\n\n        # cluster: raise sims for target-class prototypes\n        cluster = 0.0\n        C = model.num_classes; Ppc = model.num_prototypes_per_class\n        for c in range(C):\n            mask = (targets == c)\n            if mask.any():\n                s, e = c*Ppc, (c+1)*Ppc\n                cluster += (1.0 - sims[mask, s:e]).mean()\n        cluster = cluster / C\n\n        # separation: lower sims for other-class prototypes\n        separation, count = 0.0, 0\n        for c in range(C):\n            mask = (targets != c)\n            if mask.any():\n                s, e = c*Ppc, (c+1)*Ppc\n                separation += F.relu(sims[mask, s:e] - 0.1).mean()\n                count += 1\n        separation = separation / max(1, count)\n\n        l1 = model.classifier.weight.sum()  # non-negative\n        total = (self.class_weight*classification_loss + self.cluster_weight*cluster +\n                 self.separation_weight*separation + self.l1_weight*l1)\n        return {\"total_loss\": total, \"classification_loss\": classification_loss, \"cluster_loss\": cluster,\n                \"separation_loss\": separation, \"l1_loss\": l1, \"processed_logits\": processed_logits}"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6MAOYorBriv"
   },
   "source": [
    "#PIP-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "NH8P7HR7BtZC"
   },
   "outputs": [],
   "source": [
    "def proto_presence_from_sim_maps(sim_maps: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    sim_maps: (B,P,H,W). Convert to per-location soft assignment z (B,H,W,P),\n",
    "    then pool presence vector p in [0,1]^P via max over spatial.\n",
    "    \"\"\"\n",
    "    Bsz,P,H,W = sim_maps.shape\n",
    "    z = F.softmax(sim_maps.permute(0,2,3,1), dim=-1)   # (B,H,W,P)\n",
    "    p = z.amax(dim=(1,2))                              # (B,P)\n",
    "    return z, p\n",
    "\n",
    "def loss_LA(z1: torch.Tensor, z2: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"Patch alignment: -mean log( dot(z1,z2) ). z*: (B,H,W,P) with sum_P=1.\"\"\"\n",
    "    dot = (z1 * z2).sum(dim=-1).clamp_min(eps)         # (B,H,W)\n",
    "    return -torch.log(dot).mean()\n",
    "\n",
    "def loss_LT(p_batch: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"tanh diversity over batch presence p: (B,P).\"\"\"\n",
    "    sum_b = p_batch.sum(dim=0)                         # (P,)\n",
    "    return -(torch.log(torch.tanh(sum_b) + eps)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihe_sBLGBv9M"
   },
   "source": [
    "#Metric utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "u6P0ohlnBxmW"
   },
   "outputs": [],
   "source": [
    "!pip -q install scikit-learn tqdm\n",
    "\n",
    "import numpy as np, sklearn.metrics as skm\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_metrics(net,\n",
    "                     dl_in,\n",
    "                     cfg,\n",
    "                     dl_ood=None,\n",
    "                     thr_sim=0.05,\n",
    "                     device=device):\n",
    "    \"\"\"\n",
    "    net      : BcosPiPNet\n",
    "    dl_in    : DataLoader (ID validation/test split)\n",
    "    cfg      : your CFG object (only Ppc is needed)\n",
    "    dl_ood   : DataLoader with *unlabelled* OoD images (optional)\n",
    "    thr_sim  : similarity threshold for sparsity\n",
    "    returns  : dict with metrics\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    correct = total = 0\n",
    "    purity_ok = purity_tot = 0\n",
    "    sparsity_list = []\n",
    "    conf_in = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(dl_in, desc=\"Eval ID\", leave=False):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            out   = net(imgs)\n",
    "            logits, sims = out[\"logits\"], out[\"similarities\"]\n",
    "\n",
    "            # ---------- accuracy ----------\n",
    "            preds = logits.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total   += labels.size(0)\n",
    "\n",
    "            # ---------- prototype purity ----------\n",
    "            top_p   = sims.argmax(1)                      # (B,)\n",
    "            proto_cls = (top_p // cfg.num_prototypes_per_class).cpu()\n",
    "            purity_ok += (proto_cls == labels.cpu()).sum().item()\n",
    "            purity_tot += labels.size(0)\n",
    "\n",
    "            # ---------- sparsity ----------\n",
    "            sparsity_list.extend((sims > thr_sim).sum(1).cpu().tolist())\n",
    "\n",
    "            # ---------- confidence (for FPR95) ----------\n",
    "            conf_in.extend(logits.softmax(1).max(1).values.cpu().tolist())\n",
    "\n",
    "    acc      = 100.0 * correct / total\n",
    "    purity   = 100.0 * purity_ok / purity_tot\n",
    "    sparsity = float(np.mean(sparsity_list))\n",
    "\n",
    "    # ---------- OoD FPR95 ----------\n",
    "    fpr95 = None\n",
    "    if dl_ood is not None:\n",
    "        conf_ood = []\n",
    "        with torch.no_grad():\n",
    "            for imgs,_ in tqdm(dl_ood, desc=\"Eval OoD\", leave=False):\n",
    "                imgs = imgs.to(device)\n",
    "                logits = net(imgs)[\"logits\"]\n",
    "                conf_ood.extend(logits.softmax(1).max(1).values.cpu().tolist())\n",
    "\n",
    "        # higher confidence ⇒ more ID-like; we invert sign for ROC so\n",
    "        # lower score = ID, higher = OoD\n",
    "        scores = -np.concatenate([conf_in, conf_ood])\n",
    "        labels = np.concatenate([np.ones(len(conf_in)), np.zeros(len(conf_ood))])\n",
    "        fpr, tpr, _ = skm.roc_curve(labels, scores)\n",
    "        try:\n",
    "            idx = np.where(tpr >= 0.95)[0][0]\n",
    "            fpr95 = 100.0 * fpr[idx]\n",
    "        except IndexError:\n",
    "            fpr95 = 100.0       # tpr never reached 95 %\n",
    "\n",
    "    metrics = {\"Accuracy (%)\"     : acc,\n",
    "               \"Purity (%)\"       : purity,\n",
    "               \"Sparsity (avg #)\" : sparsity}\n",
    "    if fpr95 is not None:\n",
    "        metrics[\"FPR95 (%)\"] = fpr95\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArM8sdgWB4Ya"
   },
   "source": [
    "#Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUxeEyLdB3Os"
   },
   "outputs": [],
   "source": "class Config:\n    dataset = \"CIFAR10\"   # or 'CIFAR100'\n    data_root = \"./data\"\n    batch_size = 1024\n    num_workers = max(2, (os.cpu_count() or 4)//2)\n    image_size = 160\n\n    # Model\n    num_classes = 10\n    num_prototypes_per_class = 10\n    prototype_shape = (128,1,1)\n    base_channels = 64\n    B = 2.5\n    patchify_stem = True\n\n    # Training\n    epochs_pretrain = 16    # Contrastive learning only (classifier frozen)\n    epochs_supervised = 100  # CE + regs + light contrastive regularization\n    lr = 1e-3\n    wd = 1e-4\n\n    # B-cos specific parameters (following official implementation)\n    use_bce = True                      # Use BCE instead of CrossEntropy\n    logit_bias = np.log(0.1/0.9)        # B-cos default bias\n    logit_temperature = 1.0             # Temperature scaling\n    \n    # Loss weights\n    class_weight = 1.0\n    cluster_weight = 0.6\n    separation_weight = 0.05\n    l1_weight = 2e-4\n    lambda_A_pre = 1.0      # Align loss weight for pretraining\n    lambda_T_pre = 0.5      # Tanh loss weight for pretraining  \n    lambda_A_sup = 0.2      # Align loss weight for supervised (regularization)\n    lambda_T_sup = 0.1      # Tanh loss weight for supervised (regularization)\n\n    # Saving\n    save_dir = \"./checkpoints\"\n    save_frequency = 10"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebc63mOVCAor"
   },
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpTwmfTTCIQE"
   },
   "outputs": [],
   "source": "def pretrain_prototypes(model, pre_loader, contrastive_crit, cfg: Config):\n    \"\"\"\n    Freeze classifier; optimize features + prototypes with contrastive learning (LA + LT).\n    \"\"\"\n    for p in model.classifier.parameters(): p.requires_grad = False\n    opt = optim.Adam(list(model.features.parameters()) + list(model.prototype_layer.parameters()),\n                     lr=cfg.lr, weight_decay=cfg.wd)\n\n    print(f\"pretraining prototypes: {cfg.epochs_pretrain} epochs (Contrastive Learning: LA+LT)\")\n    for ep in range(cfg.epochs_pretrain):\n        model.train()\n        ep_loss = ep_LA = ep_LT = 0.0\n        pbar = tqdm(pre_loader, desc=f\"Pretrain {ep+1}/{cfg.epochs_pretrain}\", ncols=120, leave=False)\n        for (x1, x2), _ in pbar:\n            x1, x2 = x1.to(device), x2.to(device)\n            opt.zero_grad()\n            \n            # Get prototype activations for both views\n            sim_maps1, _ = model.get_prototype_activations(x1)\n            sim_maps2, _ = model.get_prototype_activations(x2)\n            \n            # Compute contrastive loss\n            loss_dict = contrastive_crit(sim_maps1, sim_maps2)\n            loss = loss_dict['total_loss']\n            \n            loss.backward()\n            opt.step()\n\n            ep_loss += loss.item()\n            ep_LA += loss_dict['align_loss'].item()\n            ep_LT += loss_dict['tanh_loss'].item()\n            pbar.set_postfix({\"Loss\": f\"{loss.item():.3f}\", \n                              \"LA\": f\"{loss_dict['align_loss'].item():.3f}\", \n                              \"LT\": f\"{loss_dict['tanh_loss'].item():.3f}\"})\n        \n        print(f\"pretrain epoch {ep+1}: Loss {ep_loss/len(pre_loader):.3f} | \"\n              f\"LA {ep_LA/len(pre_loader):.3f} | LT {ep_LT/len(pre_loader):.3f}\")\n    \n    # unfreeze classifier for supervised training\n    for p in model.classifier.parameters(): p.requires_grad = True\n\ndef train_supervised(model,\n                     train_loader,\n                     val_loader,\n                     test_loader,\n                     supervised_crit,\n                     opt,\n                     sch,\n                     cfg,\n                     ood_loader=None):\n    \"\"\"\n    Supervised phase with metric tracking.\n    Uses contrastive learning regularization instead of prototype pushing.\n    Updated for B-cos BCE loss.\n    \"\"\"\n    os.makedirs(cfg.save_dir, exist_ok=True)\n\n    history = {\n        \"train_loss\": [], \"train_acc\": [],\n        \"val_loss\":   [], \"val_acc\":   [],\n        \"purity\":     [], \"sparsity\":  [], \"fpr95\": [],\n        \"lr\": [], \"epoch_times\": []\n    }\n    best_val = 0.0\n\n    print(f\"supervised training for {cfg.epochs_supervised} epochs...\")\n    for ep in range(cfg.epochs_supervised):\n        t0 = time.time()\n        # -------------------- TRAIN --------------------\n        model.train()\n        tr_loss = tr_correct = tr_total = 0\n\n        pbar = tqdm(train_loader,\n                    desc=f\"train {ep+1}/{cfg.epochs_supervised}\",\n                    ncols=120, leave=False)\n        for imgs, targets in pbar:\n            imgs, targets = imgs.to(device), targets.to(device)\n            opt.zero_grad()\n            out = model(imgs)\n\n            # core PiP-Net losses (updated for B-cos BCE)\n            losses = supervised_crit(out, targets, model)\n\n            # lightweight contrastive regularization to keep prototype learning stable\n            sm  = out[\"sim_maps\"]\n            sm2 = torch.roll(sm, shifts=1, dims=-1)               # shifted view for regularization\n            z1, _ = proto_presence_from_sim_maps(sm)\n            z2, _ = proto_presence_from_sim_maps(sm2)\n            LA_s  = loss_LA(z1, z2)\n            _, p  = proto_presence_from_sim_maps(sm)\n            LT_s  = loss_LT(p)\n\n            extra = cfg.lambda_A_sup*LA_s + cfg.lambda_T_sup*LT_s\n            loss  = losses[\"total_loss\"] + extra\n\n            loss.backward()\n            opt.step()\n\n            tr_loss   += loss.item()\n            tr_correct += (out[\"logits\"].argmax(1) == targets).sum().item()\n            tr_total  += targets.size(0)\n            pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\",\n                              \"Acc\":  f\"{100.0*tr_correct/tr_total:.2f}%\"})\n\n        # -------------------- VALIDATION --------------------\n        model.eval()\n        val_loss = val_correct = val_total = 0\n        with torch.no_grad():\n            for imgs, targets in val_loader:\n                imgs, targets = imgs.to(device), targets.to(device)\n                out   = model(imgs)\n                loss_dict = supervised_crit(out, targets, model)\n                val_loss += loss_dict[\"total_loss\"].item()\n                val_correct += (out[\"logits\"].argmax(1) == targets).sum().item()\n                val_total   += targets.size(0)\n\n        # -------------------- PER-EPOCH METRICS --------------------\n        metrics_val = evaluate_metrics(model, val_loader, cfg, dl_ood=ood_loader)\n        print(f\"val metrics: {metrics_val}\")\n\n        epoch_time = time.time() - t0\n        train_loss_avg = tr_loss / len(train_loader)\n        train_acc      = 100. * tr_correct / tr_total\n        val_loss_avg   = val_loss / len(val_loader)\n        val_acc        = 100. * val_correct / val_total\n\n        # ---------- history ----------\n        history[\"train_loss\"].append(train_loss_avg)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_loss\"].append(val_loss_avg)\n        history[\"val_acc\"].append(val_acc)\n        history[\"purity\"].append(metrics_val[\"Purity (%)\"])\n        history[\"sparsity\"].append(metrics_val[\"Sparsity (avg #)\"])\n        history[\"fpr95\"].append(metrics_val.get(\"FPR95 (%)\", None))\n        history[\"lr\"].append(opt.param_groups[0][\"lr\"])\n        history[\"epoch_times\"].append(epoch_time)\n\n        print(f\"train {train_loss_avg:.4f}/{train_acc:.2f}% | \"\n              f\"val {val_loss_avg:.4f}/{val_acc:.2f}% | \"\n              f\"LR {opt.param_groups[0]['lr']:.3g} | \"\n              f\"time {timedelta(seconds=int(epoch_time))}\")\n\n        # -------------------- CHECKPOINTS --------------------\n        if val_acc > best_val:\n            best_val = val_acc\n            torch.save({\n                \"epoch\": ep + 1,\n                \"model_state_dict\": model.state_dict(),\n                \"optimizer_state_dict\": opt.state_dict(),\n                \"val_acc\": val_acc,\n                \"config\": cfg.__dict__,\n                \"history\": history\n            }, os.path.join(cfg.save_dir, \"best_model.pth\"))\n            print(f\"new best model saved! Val Acc: {val_acc:.2f}%\")\n\n        if (ep + 1) % cfg.save_frequency == 0:\n            torch.save({\n                \"epoch\": ep + 1,\n                \"model_state_dict\": model.state_dict(),\n                \"optimizer_state_dict\": opt.state_dict(),\n                \"history\": history,\n                \"config\": cfg.__dict__,\n            }, os.path.join(cfg.save_dir, f\"checkpoint_epoch_{ep+1}.pth\"))\n            print(f\"checkpoint saved at epoch {ep+1}\")\n\n        sch.step()\n\n    # -------------------- TEST / OoD METRICS --------------------\n    metrics_test = evaluate_metrics(model, test_loader, cfg, dl_ood=ood_loader)\n    print(\"test / OoD metrics:\", metrics_test)\n\n    print(f\"done. Best Val Acc: {best_val:.2f}%\")\n    return model, history\n\n# Two-view transform with shared geometry, different color jitter\nclass TwoViewSharedGeom:\n    def __init__(self, image_size, max_rot=10, flip_p=0.5,\n                 cj_s1=0.2, cj_s2=0.4):\n        self.image_size = image_size\n        self.max_rot = max_rot\n        self.flip_p = flip_p\n        self.cj1 = T.ColorJitter(brightness=cj_s1, contrast=cj_s1, saturation=cj_s1, hue=0.05)\n        self.cj2 = T.ColorJitter(brightness=cj_s2, contrast=cj_s2, saturation=cj_s2, hue=0.1)\n    def _geom(self, img, flip, angle):\n        img = TF.resize(img, [self.image_size, self.image_size], antialias=True)\n        if flip: img = TF.hflip(img)\n        if abs(angle) > 1e-3: img = TF.rotate(img, angle)\n        return img\n    def __call__(self, img):\n        flip = random.random() < self.flip_p\n        angle = random.uniform(-self.max_rot, self.max_rot)\n        # shared geometry\n        i1 = self._geom(img, flip, angle)\n        i2 = self._geom(img, flip, angle)\n        # distinct colors\n        i1 = self.cj1(i1); i2 = self.cj2(i2)\n        return TF.to_tensor(i1), TF.to_tensor(i2)\n\ndef get_dataloaders(cfg: Config):\n    # Supervised train/val/test transforms\n    train_tf = T.Compose([\n        T.Resize((cfg.image_size, cfg.image_size)),\n        T.RandomHorizontalFlip(0.5),\n        T.RandomRotation(10),\n        T.ColorJitter(0.2,0.2,0.2,0.1),\n        T.ToTensor(),\n    ])\n    test_tf = T.Compose([T.Resize((cfg.image_size, cfg.image_size)), T.ToTensor()])\n\n    if cfg.dataset.upper() == \"CIFAR10\":\n        train_set = torchvision.datasets.CIFAR10(cfg.data_root, train=True, download=True, transform=train_tf)\n        test_set  = torchvision.datasets.CIFAR10(cfg.data_root, train=False, download=True, transform=test_tf)\n        cfg.num_classes = 10\n    else:\n        train_set = torchvision.datasets.CIFAR100(cfg.data_root, train=True, download=True, transform=train_tf)\n        test_set  = torchvision.datasets.CIFAR100(cfg.data_root, train=False, download=True, transform=test_tf)\n        cfg.num_classes = 100\n\n    # Split\n    n_train = int(0.8 * len(train_set))\n    n_val = len(train_set) - n_train\n    train_sub, val_sub = random_split(train_set, [n_train, n_val], generator=torch.Generator().manual_seed(42))\n\n    train_loader = DataLoader(train_sub, batch_size=cfg.batch_size, shuffle=True,\n                              num_workers=cfg.num_workers, pin_memory=True)\n    val_loader   = DataLoader(val_sub,   batch_size=cfg.batch_size, shuffle=False,\n                              num_workers=cfg.num_workers, pin_memory=True)\n    test_loader  = DataLoader(test_set,  batch_size=cfg.batch_size, shuffle=False,\n                              num_workers=cfg.num_workers, pin_memory=True)\n\n    # Two-view dataloader for contrastive pretraining (same images, different transform)\n    two_view = TwoViewSharedGeom(cfg.image_size)\n    if cfg.dataset.upper() == \"CIFAR10\":\n        pre_set = torchvision.datasets.CIFAR10(cfg.data_root, train=True, download=False, transform=two_view)\n    else:\n        pre_set = torchvision.datasets.CIFAR100(cfg.data_root, train=True, download=False, transform=two_view)\n\n    pre_loader = DataLoader(pre_set, batch_size=cfg.batch_size, shuffle=True,\n                            num_workers=cfg.num_workers, pin_memory=True, drop_last=True)\n    return train_loader, val_loader, test_loader, pre_loader"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8ryA5X3CJ-3"
   },
   "source": [
    "#Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0nzzmK3CK4b",
    "outputId": "1f91774f-1f42-4fc2-fa04-b81b421eaa5b"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    cfg = Config()\n",
    "    train_loader, val_loader, test_loader, pre_loader = get_dataloaders(cfg)\n",
    "    print(f\"data ready. train:{len(train_loader.dataset)} Val:{len(val_loader.dataset)} Test:{len(test_loader.dataset)}\")\n",
    "\n",
    "    model, supervised_crit, contrastive_crit, opt, sch = setup(cfg)\n",
    "    print(\"phase 1/2: contrastive pretraining (LA+LT)\")\n",
    "    pretrain_prototypes(model, pre_loader, contrastive_crit, cfg)\n",
    "\n",
    "    print(\"phase 2/2: supervised training with contrastive regularization\")\n",
    "    model, history = train_supervised(model, train_loader, val_loader, test_loader, supervised_crit, opt, sch, cfg)\n",
    "\n",
    "    return model, history, cfg\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, history, config = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtRPJf4uQm6p"
   },
   "source": [
    "#load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6GJgY49_QpGN",
    "outputId": "68f8f636-1365-42db-f76e-25e478604dd2"
   },
   "outputs": [],
   "source": [
    "# Run Comprehensive Prototype Analysis\n",
    "\n",
    "# If you have a trained model, run this comprehensive analysis\n",
    "try:\n",
    "    if 'model' in locals() and model is not None:\n",
    "        print(\"🚀 Starting comprehensive prototype analysis...\")\n",
    "        run_comprehensive_prototype_analysis(model, cfg, test_loader)\n",
    "        \n",
    "        # Also demonstrate individual functions\n",
    "        print(f\"\\n\\n🎯 QUICK PROTOTYPE EXAMPLES\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Show how to analyze a specific prototype\n",
    "        print(\"Example 1: Analyzing prototype 5...\")\n",
    "        visualize_prototype_analysis(model, cfg, test_loader, 5)\n",
    "        \n",
    "        print(\"Example 2: Showing patch grid for prototype 15...\")\n",
    "        visualize_prototype_patches_grid(model, cfg, test_loader, 15, num_patches=6)\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠️  No trained model loaded. Please run the training first or load a checkpoint.\")\n",
    "        print(\"💡 You can still examine the visualization functions!\")\n",
    "        \n",
    "except NameError:\n",
    "    print(\"⚠️  Model not found. Please load a trained model first.\")\n",
    "    print(\"💡 Example usage:\")\n",
    "    print(\"\"\"\n",
    "    # Load your model\n",
    "    model = BcosPiPNet(...)\n",
    "    checkpoint = torch.load('path/to/checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Run analysis\n",
    "    run_comprehensive_prototype_analysis(model, cfg, test_loader)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BYgXtxWCOE_"
   },
   "source": [
    "# Additional Prototype Visualization Functions\n",
    "\n",
    "def visualize_prototype_patches_grid(model, cfg, test_loader, prototype_idx, num_patches=9):\n",
    "    \"\"\"\n",
    "    Show a grid of top-activating patches for a specific prototype\n",
    "    \"\"\"\n",
    "    print(f\"Finding top {num_patches} patches for prototype {prototype_idx}...\")\n",
    "    \n",
    "    # Extract top patches for this prototype\n",
    "    patches = extract_prototype_patches(model, test_loader, prototype_idx, num_patches)\n",
    "    \n",
    "    if not patches:\n",
    "        print(f\"No activating patches found for prototype {prototype_idx}\")\n",
    "        return None\n",
    "    \n",
    "    # Create grid visualization\n",
    "    cols = 3\n",
    "    rows = (len(patches) + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*4))\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # Get prototype info\n",
    "    prototype = model.prototype_layer.prototypes[prototype_idx]\n",
    "    proto_class = prototype_idx // cfg.num_prototypes_per_class\n",
    "    \n",
    "    fig.suptitle(f'Prototype {prototype_idx} (Class {proto_class}) - Top Activating Patches', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, patch_info in enumerate(patches):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Show the image patch\n",
    "        img = patch_info['image']\n",
    "        ax.imshow(to_numpy_img(img))\n",
    "        ax.set_title(f'Similarity: {patch_info[\"similarity\"]:.3f}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(patches), rows * cols):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_class_prototypes_summary(model, cfg, test_loader, class_idx, max_prototypes=5):\n",
    "    \"\"\"\n",
    "    Show summary of prototypes for a specific class\n",
    "    \"\"\"\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"CLASS {class_idx} PROTOTYPE SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get prototypes for this class\n",
    "    start_idx = class_idx * cfg.num_prototypes_per_class\n",
    "    end_idx = start_idx + cfg.num_prototypes_per_class\n",
    "    num_to_show = min(max_prototypes, cfg.num_prototypes_per_class)\n",
    "    \n",
    "    for i in range(num_to_show):\n",
    "        proto_idx = start_idx + i\n",
    "        print(f\"\\n--- Prototype {proto_idx} (Class {class_idx}, Proto {i+1}/{cfg.num_prototypes_per_class}) ---\")\n",
    "        \n",
    "        # Show single prototype analysis\n",
    "        visualize_prototype_analysis(model, cfg, test_loader, proto_idx)\n",
    "\n",
    "def analyze_prototype_diversity(model, cfg):\n",
    "    \"\"\"\n",
    "    Analyze the diversity and statistics of learned prototypes\n",
    "    \"\"\"\n",
    "    prototypes = model.prototype_layer.prototypes.data  # [P, C, H, W]\n",
    "    P, C, H, W = prototypes.shape\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"PROTOTYPE DIVERSITY ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total prototypes: {P}\")\n",
    "    print(f\"Prototype shape: [{C}, {H}, {W}]\")\n",
    "    print(f\"Classes: {cfg.num_classes}\")\n",
    "    print(f\"Prototypes per class: {cfg.num_prototypes_per_class}\")\n",
    "    \n",
    "    # Flatten prototypes for analysis\n",
    "    proto_flat = prototypes.view(P, -1)  # [P, C*H*W]\n",
    "    \n",
    "    # Compute statistics\n",
    "    proto_means = proto_flat.mean(dim=1)  # [P]\n",
    "    proto_stds = proto_flat.std(dim=1)    # [P]\n",
    "    proto_norms = proto_flat.norm(dim=1)  # [P]\n",
    "    \n",
    "    # Compute pairwise similarities between prototypes\n",
    "    proto_normalized = F.normalize(proto_flat, dim=1)\n",
    "    similarity_matrix = torch.mm(proto_normalized, proto_normalized.t())  # [P, P]\n",
    "    \n",
    "    # Remove diagonal (self-similarity)\n",
    "    mask = torch.eye(P).bool()\n",
    "    off_diagonal_sims = similarity_matrix[~mask]\n",
    "    \n",
    "    print(f\"\\nPrototype Statistics:\")\n",
    "    print(f\"  Mean activation: {proto_means.mean():.4f} ± {proto_means.std():.4f}\")\n",
    "    print(f\"  Mean std: {proto_stds.mean():.4f} ± {proto_stds.std():.4f}\")\n",
    "    print(f\"  Mean norm: {proto_norms.mean():.4f} ± {proto_norms.std():.4f}\")\n",
    "    \n",
    "    print(f\"\\nPrototype Diversity:\")\n",
    "    print(f\"  Mean pairwise similarity: {off_diagonal_sims.mean():.4f}\")\n",
    "    print(f\"  Max pairwise similarity: {off_diagonal_sims.max():.4f}\")\n",
    "    print(f\"  Min pairwise similarity: {off_diagonal_sims.min():.4f}\")\n",
    "    \n",
    "    # Plot similarity distribution\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Histogram of pairwise similarities\n",
    "    ax1.hist(off_diagonal_sims.cpu().numpy(), bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_xlabel('Pairwise Cosine Similarity')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Distribution of Prototype Similarities')\n",
    "    ax1.axvline(off_diagonal_sims.mean().item(), color='red', linestyle='--', \n",
    "                label=f'Mean: {off_diagonal_sims.mean():.3f}')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Prototype norms\n",
    "    ax2.hist(proto_norms.cpu().numpy(), bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
    "    ax2.set_xlabel('Prototype L2 Norm')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Distribution of Prototype Norms')\n",
    "    ax2.axvline(proto_norms.mean().item(), color='red', linestyle='--',\n",
    "                label=f'Mean: {proto_norms.mean():.3f}')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'similarity_matrix': similarity_matrix,\n",
    "        'mean_similarity': off_diagonal_sims.mean().item(),\n",
    "        'mean_norm': proto_norms.mean().item(),\n",
    "        'diversity_score': 1 - off_diagonal_sims.mean().item()  # Higher = more diverse\n",
    "    }\n",
    "\n",
    "# Enhanced demonstration\n",
    "def run_comprehensive_prototype_analysis(model, cfg, test_loader):\n",
    "    \"\"\"Run a comprehensive analysis of learned prototypes\"\"\"\n",
    "    \n",
    "    print(\"🔍 COMPREHENSIVE PROTOTYPE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Overall prototype diversity analysis\n",
    "    diversity_stats = analyze_prototype_diversity(model, cfg)\n",
    "    print(f\"\\n📊 Diversity Score: {diversity_stats['diversity_score']:.4f} (higher = more diverse)\")\n",
    "    \n",
    "    # 2. Show prototypes from first few classes\n",
    "    for class_idx in range(min(2, cfg.num_classes)):\n",
    "        visualize_class_prototypes_summary(model, cfg, test_loader, class_idx, max_prototypes=2)\n",
    "    \n",
    "    # 3. Show patch grids for a few interesting prototypes\n",
    "    print(f\"\\n🔍 DETAILED PATCH ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Pick some prototypes to analyze in detail\n",
    "    interesting_prototypes = []\n",
    "    \n",
    "    # Pick one prototype per class for detailed analysis\n",
    "    for class_idx in range(min(3, cfg.num_classes)):\n",
    "        proto_idx = class_idx * cfg.num_prototypes_per_class\n",
    "        interesting_prototypes.append(proto_idx)\n",
    "    \n",
    "    for proto_idx in interesting_prototypes[:2]:  # Limit to avoid too much output\n",
    "        visualize_prototype_patches_grid(model, cfg, test_loader, proto_idx, num_patches=6)\n",
    "    \n",
    "    print(\"\\n✅ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "ayr7PNHOUizJ",
    "outputId": "6a5b3bf1-cceb-485f-df51-18fd01ec1ef9"
   },
   "outputs": [],
   "source": [
    "def grad_to_img_bcos(img_6ch, linear_mapping, smooth=3, alpha_percentile=99.5):\n",
    "    \"\"\"\n",
    "    Computing color image from dynamic linear mapping of B-cos models.\n",
    "    Uses the proper B-cos grad_to_img function from /B-cos/interpretability/utils.py\n",
    "    \n",
    "    Args:\n",
    "        img_6ch: Original 6-channel input image [r,g,b,1-r,1-g,1-b] - shape (6,H,W)\n",
    "        linear_mapping: linear mapping from B-cos model - shape (6,H,W)\n",
    "        smooth: kernel size for smoothing the alpha values\n",
    "        alpha_percentile: cut-off percentile for the alpha value\n",
    "    \n",
    "    Returns:\n",
    "        RGBA image explanation of the B-cos model - shape (H,W,4)\n",
    "    \"\"\"\n",
    "    # Import the proper B-cos explanation utilities\n",
    "    try:\n",
    "        sys.path.append('./B-cos')\n",
    "        from interpretability.utils import grad_to_img\n",
    "        \n",
    "        # Use the official B-cos grad_to_img function\n",
    "        return grad_to_img(img_6ch, linear_mapping, smooth=smooth, alpha_percentile=alpha_percentile)\n",
    "        \n",
    "    except ImportError:\n",
    "        # Fallback implementation if B-cos utils not available\n",
    "        print(\"Warning: Using fallback B-cos explanation (B-cos utils not found)\")\n",
    "        \n",
    "        # Ensure tensors are on CPU and detached\n",
    "        if hasattr(img_6ch, 'detach'):\n",
    "            img_6ch = img_6ch.detach().cpu()\n",
    "        if hasattr(linear_mapping, 'detach'):\n",
    "            linear_mapping = linear_mapping.detach().cpu()\n",
    "        \n",
    "        # Shape should be [6, H, W]\n",
    "        if len(img_6ch.shape) == 4:\n",
    "            img_6ch = img_6ch[0]\n",
    "        if len(linear_mapping.shape) == 4:\n",
    "            linear_mapping = linear_mapping[0]\n",
    "        \n",
    "        # Compute contributions: summing over channel dimension gives contribution map per location\n",
    "        contribs = (img_6ch * linear_mapping).sum(0, keepdim=True)  # [1, H, W]\n",
    "        contribs = contribs[0]  # [H, W]\n",
    "        \n",
    "        # Normalize each pixel vector s.t. max entry is 1, maintaining direction\n",
    "        rgb_grad = (linear_mapping / (linear_mapping.abs().max(0, keepdim=True)[0] + 1e-12))\n",
    "        \n",
    "        # Clip off values below 0 (set negatively weighted channels to 0)\n",
    "        rgb_grad = rgb_grad.clamp(0)\n",
    "        \n",
    "        # Normalize s.t. each pair (e.g., r and 1-r) sums to 1, use only RGB values\n",
    "        rgb_grad = rgb_grad[:3] / (rgb_grad[:3] + rgb_grad[3:] + 1e-12)\n",
    "        \n",
    "        # Set alpha value to the strength (L2 norm) of each location's gradient\n",
    "        alpha = linear_mapping.norm(p=2, dim=0, keepdim=True)  # [1, H, W]\n",
    "        \n",
    "        # Only show positive contributions\n",
    "        alpha = torch.where(contribs[None] < 0, torch.zeros_like(alpha) + 1e-12, alpha)\n",
    "        \n",
    "        # Apply smoothing if requested\n",
    "        if smooth > 1:\n",
    "            alpha = F.avg_pool2d(alpha, smooth, stride=1, padding=(smooth-1)//2)\n",
    "        \n",
    "        # Convert to numpy and normalize alpha\n",
    "        alpha = alpha.numpy()\n",
    "        alpha = (alpha / np.percentile(alpha, alpha_percentile)).clip(0, 1)\n",
    "        \n",
    "        # Convert RGB gradients to numpy\n",
    "        rgb_grad = rgb_grad.numpy()\n",
    "        \n",
    "        # Combine RGB + Alpha: shape [4, H, W]\n",
    "        rgba_grad = np.concatenate([rgb_grad, alpha], axis=0)\n",
    "        \n",
    "        # Reshape to [H, W, 4] for display\n",
    "        grad_image = rgba_grad.transpose((1, 2, 0))\n",
    "        \n",
    "        return grad_image"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}