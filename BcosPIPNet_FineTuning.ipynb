{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Fine-tuning B-cos PIP-Net with Scoring-Sheet Classification\n",
    "\n",
    "This notebook fine-tunes the pre-trained B-cos PIP-Net model for classification tasks using a scoring-sheet approach.\n",
    "\n",
    "## Key Features:\n",
    "- **Scoring-Sheet Classification**: Linear classifier where weights indicate prototype-class relevance\n",
    "- **Negative Log-Likelihood Loss**: Standard classification loss with softmax activation\n",
    "- **Prototype Purity Evaluation**: Measures consistency of prototype activations\n",
    "- **Interpretable Results**: Class scores computed as weighted sum of prototype presences\n",
    "\n",
    "## Mathematical Foundation:\n",
    "- Class score for class j: `score_j = Σ(p_i * w_ij)` where p_i is prototype presence and w_ij is relevance weight\n",
    "- Final predictions: `softmax(scores)` to get class confidence scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install tqdm tensorboard\n",
    "!pip install matplotlib seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install pillow pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository (if not already done)\n",
    "import os\n",
    "if not os.path.exists('improved-Bcos-PIPNet'):\n",
    "    !git clone https://github.com/your-username/improved-Bcos-PIPNet.git\n",
    "%cd improved-Bcos-PIPNet\n",
    "\n",
    "# Initialize submodules\n",
    "!git submodule update --init --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_paths"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add source directories to Python path\n",
    "sys.path.append('src')\n",
    "sys.path.append('B-cos')\n",
    "sys.path.append('PIPNet')\n",
    "\n",
    "# Verify paths\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "print(\"Fine-tuning script exists:\", os.path.exists('src/train_finetune.py'))\n",
    "print(\"Classifier module exists:\", os.path.exists('src/finetune_classifier.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_pretrained"
   },
   "source": [
    "## 2. Upload Pre-trained Model\n",
    "\n",
    "Upload your pre-trained B-cos PIP-Net checkpoint from the previous pre-training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_checkpoint"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Create checkpoints directory\n",
    "os.makedirs('./checkpoints/pretrained', exist_ok=True)\n",
    "\n",
    "print(\"Please upload your pre-trained B-cos PIP-Net checkpoint (.pth file):\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move uploaded file to checkpoints directory\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.pth'):\n",
    "        os.rename(filename, f'./checkpoints/pretrained/{filename}')\n",
    "        pretrained_path = f'./checkpoints/pretrained/{filename}'\n",
    "        print(f\"Pre-trained model saved to: {pretrained_path}\")\n",
    "        break\n",
    "else:\n",
    "    # Use a default path if no file uploaded (for testing)\n",
    "    pretrained_path = './checkpoints/pretrained/final_model.pth'\n",
    "    print(f\"No pre-trained model uploaded. Will use: {pretrained_path}\")\n",
    "    print(\"Note: Make sure to upload your pre-trained checkpoint for actual fine-tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_modules"
   },
   "source": [
    "## 3. Import Modules and Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import fine-tuning modules\n",
    "from src.finetune_classifier import (\n",
    "    create_scoring_sheet_classifier,\n",
    "    FineTuningLoss,\n",
    "    evaluate_model,\n",
    "    compute_prototype_purity\n",
    ")\n",
    "from src.datasets import SixChannelDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_pretrained_loading"
   },
   "outputs": [],
   "source": [
    "# Test loading pre-trained model (if checkpoint exists)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if os.path.exists(pretrained_path):\n",
    "    print(f\"Loading pre-trained model from: {pretrained_path}\")\n",
    "    \n",
    "    # Load checkpoint to check contents\n",
    "    checkpoint = torch.load(pretrained_path, map_location='cpu')\n",
    "    print(f\"Checkpoint keys: {list(checkpoint.keys())}\")\n",
    "    \n",
    "    if 'args' in checkpoint:\n",
    "        pretrained_args = checkpoint['args']\n",
    "        print(f\"Pre-trained model config:\")\n",
    "        print(f\"  Backbone: {pretrained_args.get('backbone', 'unknown')}\")\n",
    "        print(f\"  Prototypes: {pretrained_args.get('num_prototypes', 'unknown')}\")\n",
    "        print(f\"  Dataset: {pretrained_args.get('dataset', 'unknown')}\")\n",
    "    \n",
    "    # Test model creation for CIFAR-10 (10 classes)\n",
    "    try:\n",
    "        test_model = create_scoring_sheet_classifier(\n",
    "            pretrained_path=pretrained_path,\n",
    "            num_classes=10,  # CIFAR-10\n",
    "            freeze_prototypes=True\n",
    "        )\n",
    "        print(f\"✓ Successfully created classifier model\")\n",
    "        print(f\"  Total parameters: {sum(p.numel() for p in test_model.parameters()):,}\")\n",
    "        print(f\"  Trainable parameters: {sum(p.numel() for p in test_model.parameters() if p.requires_grad):,}\")\n",
    "        del test_model  # Free memory\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error creating model: {e}\")\n",
    "else:\n",
    "    print(f\"Pre-trained model not found at: {pretrained_path}\")\n",
    "    print(\"Please upload your pre-trained checkpoint first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset_section"
   },
   "source": [
    "## 4. Dataset Setup for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_dataset"
   },
   "outputs": [],
   "source": [
    "# Configuration for fine-tuning\n",
    "class FineTuningConfig:\n",
    "    # Dataset\n",
    "    dataset = 'cifar10'  # Change to 'cub' for CUB-200-2011\n",
    "    data_dir = './data'\n",
    "    batch_size = 64  # Reduced for Colab\n",
    "    num_workers = 2\n",
    "    img_size = 224  # For CUB dataset\n",
    "    \n",
    "    # Model\n",
    "    freeze_prototypes = True  # Freeze prototype learning components\n",
    "    \n",
    "    # Training\n",
    "    epochs = 30  # Reduced for demo\n",
    "    lr = 1e-4  # Learning rate for classifier\n",
    "    lr_backbone = 1e-5  # Learning rate for backbone (if not frozen)\n",
    "    weight_decay = 1e-4\n",
    "    warmup_epochs = 3\n",
    "    \n",
    "    # Loss weights\n",
    "    nll_weight = 1.0\n",
    "    l1_weight = 0.0001  # L1 regularization on classifier weights\n",
    "    orthogonal_weight = 0.0  # Orthogonal regularization\n",
    "    \n",
    "    # Device and logging\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    log_interval = 20\n",
    "    eval_interval = 5\n",
    "    \n",
    "    # Directories\n",
    "    log_dir = './logs/colab_finetune'\n",
    "    save_dir = './checkpoints/colab_finetune'\n",
    "    \n",
    "    # Other\n",
    "    seed = 42\n",
    "\n",
    "args = FineTuningConfig()\n",
    "print(f\"Fine-tuning configuration:\")\n",
    "print(f\"  Dataset: {args.dataset}\")\n",
    "print(f\"  Device: {args.device}\")\n",
    "print(f\"  Epochs: {args.epochs}\")\n",
    "print(f\"  Batch size: {args.batch_size}\")\n",
    "print(f\"  Freeze prototypes: {args.freeze_prototypes}\")\n",
    "print(f\"  Learning rate: {args.lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataloaders"
   },
   "outputs": [],
   "source": [
    "# Create dataloaders for CIFAR-10\n",
    "print(\"Setting up CIFAR-10 dataset for fine-tuning...\")\n",
    "\n",
    "# Data transforms with augmentation for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], \n",
    "                       std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], \n",
    "                       std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 datasets\n",
    "train_dataset_base = torchvision.datasets.CIFAR10(\n",
    "    root=args.data_dir, train=True, download=True, transform=train_transform\n",
    ")\n",
    "test_dataset_base = torchvision.datasets.CIFAR10(\n",
    "    root=args.data_dir, train=False, download=True, transform=test_transform\n",
    ")\n",
    "\n",
    "# Wrap with 6-channel transformation\n",
    "train_dataset = SixChannelDataset(train_dataset_base)\n",
    "test_dataset = SixChannelDataset(test_dataset_base)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "    num_workers=args.num_workers, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "    num_workers=args.num_workers, pin_memory=True\n",
    ")\n",
    "\n",
    "num_classes = 10  # CIFAR-10\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "# Test data loading\n",
    "sample_input, sample_label = next(iter(train_loader))\n",
    "print(f\"Sample input shape: {sample_input.shape} (6-channel)\")\n",
    "print(f\"Sample label shape: {sample_label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_directories"
   },
   "outputs": [],
   "source": [
    "# Create directories\n",
    "os.makedirs(args.log_dir, exist_ok=True)\n",
    "os.makedirs(args.save_dir, exist_ok=True)\n",
    "print(f\"Created directories: {args.log_dir}, {args.save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_setup"
   },
   "source": [
    "## 5. Model Setup and Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_model"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "# Create the fine-tuning model\n",
    "print(\"Creating scoring-sheet classifier...\")\n",
    "model = create_scoring_sheet_classifier(\n",
    "    pretrained_path=pretrained_path,\n",
    "    num_classes=num_classes,\n",
    "    freeze_prototypes=args.freeze_prototypes\n",
    ").to(args.device)\n",
    "\n",
    "print(f\"Model created successfully!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Number of prototypes: {model.num_prototypes}\")\n",
    "print(f\"Number of classes: {model.num_classes}\")\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(2, 6, 32, 32).to(args.device)\n",
    "    proto_features, pooled_features, logits, class_scores = model(test_input)\n",
    "    \n",
    "    print(f\"\\nForward pass test:\")\n",
    "    print(f\"  Prototype features: {proto_features.shape}\")\n",
    "    print(f\"  Pooled features: {pooled_features.shape}\")\n",
    "    print(f\"  Logits: {logits.shape}\")\n",
    "    print(f\"  Class scores: {class_scores.shape}\")\n",
    "    print(f\"  Class scores sum: {class_scores.sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_architecture"
   },
   "outputs": [],
   "source": [
    "# Visualize the scoring sheet concept\n",
    "print(\"Scoring-Sheet Classification Explanation:\")\n",
    "print(\"==========================================\")\n",
    "print(\"1. Input image → 6-channel encoding [r,g,b,1-r,1-g,1-b]\")\n",
    "print(\"2. B-cos backbone → Extract interpretable features\")\n",
    "print(\"3. Prototype layer → Generate prototype activations p_i\")\n",
    "print(\"4. Global pooling → Get prototype presence scores\")\n",
    "print(\"5. Linear classifier → Compute class scores: score_j = Σ(p_i * w_ij)\")\n",
    "print(\"6. Softmax → Convert to class probabilities\")\n",
    "print(\"\\nWhere:\")\n",
    "print(\"  - p_i: presence score of prototype i\")\n",
    "print(\"  - w_ij: learned relevance weight of prototype i to class j\")\n",
    "print(\"  - The classifier weights form a 'scoring sheet' showing prototype-class relationships\")\n",
    "\n",
    "# Show current classifier weights (before training)\n",
    "classifier_weights = model.get_prototype_class_relevance()\n",
    "print(f\"\\nClassifier weight matrix shape: {classifier_weights.shape}\")\n",
    "print(f\"(rows = prototypes, columns = classes)\")\n",
    "print(f\"Weight statistics:\")\n",
    "print(f\"  Mean: {classifier_weights.mean().item():.4f}\")\n",
    "print(f\"  Std: {classifier_weights.std().item():.4f}\")\n",
    "print(f\"  Min: {classifier_weights.min().item():.4f}\")\n",
    "print(f\"  Max: {classifier_weights.max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_setup"
   },
   "source": [
    "## 6. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_training"
   },
   "outputs": [],
   "source": [
    "# Create loss function\n",
    "criterion = FineTuningLoss(\n",
    "    nll_weight=args.nll_weight,\n",
    "    l1_weight=args.l1_weight,\n",
    "    orthogonal_weight=args.orthogonal_weight\n",
    ")\n",
    "\n",
    "# Create optimizer\n",
    "if args.freeze_prototypes:\n",
    "    # Only optimize classifier\n",
    "    optimizer = optim.AdamW(model.classifier.parameters(), \n",
    "                          lr=args.lr, weight_decay=args.weight_decay)\n",
    "    print(\"Optimizer: Only training classifier (prototypes frozen)\")\n",
    "else:\n",
    "    # Different learning rates for classifier and backbone\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.classifier.parameters(), 'lr': args.lr},\n",
    "        {'params': list(model.backbone.parameters()) + list(model.prototype_layer.parameters()), \n",
    "         'lr': args.lr_backbone}\n",
    "    ], weight_decay=args.weight_decay)\n",
    "    print(\"Optimizer: Training classifier + backbone with different learning rates\")\n",
    "\n",
    "print(f\"Loss function: NLL + L1({args.l1_weight}) + Orthogonal({args.orthogonal_weight})\")\n",
    "print(f\"Learning rate: {args.lr}\")\n",
    "print(f\"Weight decay: {args.weight_decay}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "learning_rate_scheduler"
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    \"\"\"Cosine learning rate schedule with warmup\"\"\"\n",
    "    if epoch < args.warmup_epochs:\n",
    "        lr_mult = epoch / args.warmup_epochs\n",
    "    else:\n",
    "        lr_mult = 0.5 * (1. + np.cos(np.pi * (epoch - args.warmup_epochs) / (args.epochs - args.warmup_epochs)))\n",
    "    \n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        if i == 0:  # Classifier parameters\n",
    "            param_group['lr'] = args.lr * lr_mult\n",
    "        else:  # Backbone parameters (if not frozen)\n",
    "            param_group['lr'] = args.lr_backbone * lr_mult\n",
    "    \n",
    "    return args.lr * lr_mult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_loop_section"
   },
   "source": [
    "## 7. Fine-tuning Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_loop",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"Starting fine-tuning...\")\n",
    "\n",
    "# Lists to store metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "prototype_purities = []\n",
    "learning_rates = []\n",
    "\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    # Adjust learning rate\n",
    "    lr = adjust_learning_rate(optimizer, epoch, args)\n",
    "    learning_rates.append(lr)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    \n",
    "    # Freeze prototype components if specified\n",
    "    if args.freeze_prototypes:\n",
    "        model.backbone.eval()\n",
    "        model.prototype_layer.eval()\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{args.epochs}')\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        inputs, targets = inputs.to(args.device), targets.to(args.device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        proto_features, pooled_features, logits, class_scores = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss_dict = criterion(logits, targets, model.classifier.weight)\n",
    "        loss = loss_dict['total_loss']\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        epoch_loss += loss.item()\n",
    "        predicted = torch.argmax(class_scores, dim=1)\n",
    "        epoch_total += targets.size(0)\n",
    "        epoch_correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        current_acc = 100. * epoch_correct / epoch_total\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "            'Acc': f'{current_acc:.2f}%',\n",
    "            'LR': f'{lr:.6f}'\n",
    "        })\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_accuracy = 100. * epoch_correct / epoch_total\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Evaluation phase\n",
    "    if epoch % args.eval_interval == 0 or epoch == args.epochs - 1:\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        eval_metrics = evaluate_model(model, test_loader, args.device, num_classes)\n",
    "        purity_metrics = compute_prototype_purity(model, test_loader, args.device, num_classes)\n",
    "        \n",
    "        test_accuracy = eval_metrics['accuracy']\n",
    "        mean_purity = purity_metrics['mean_purity']\n",
    "        \n",
    "        test_accuracies.append(test_accuracy)\n",
    "        prototype_purities.append(mean_purity)\n",
    "        \n",
    "        # Check if best model\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            print(f\"★ New best accuracy: {best_accuracy:.2f}%\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss={avg_loss:.4f}, Train Acc={train_accuracy:.2f}%, \"\n",
    "              f\"Test Acc={test_accuracy:.2f}%, Purity={mean_purity:.3f}\")\n",
    "        print(f\"  Active prototypes/sample: {eval_metrics['active_prototypes_per_sample']:.1f}\")\n",
    "        print(f\"  High purity prototypes: {purity_metrics['high_purity_prototypes']}\")\n",
    "        print(f\"  Unused prototypes: {eval_metrics['num_unused_prototypes']}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={avg_loss:.4f}, Train Acc={train_accuracy:.2f}%, LR={lr:.6f}\")\n",
    "\n",
    "print(f\"\\nFine-tuning completed!\")\n",
    "print(f\"Best test accuracy: {best_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results_section"
   },
   "source": [
    "## 8. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_training_curves"
   },
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training loss\n",
    "axes[0,0].plot(train_losses)\n",
    "axes[0,0].set_title('Training Loss')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Loss')\n",
    "axes[0,0].grid(True)\n",
    "\n",
    "# Training accuracy\n",
    "axes[0,1].plot(train_accuracies, label='Train', color='blue')\n",
    "if test_accuracies:\n",
    "    eval_epochs = [i * args.eval_interval for i in range(len(test_accuracies))]\n",
    "    if len(eval_epochs) != len(test_accuracies):\n",
    "        eval_epochs = list(range(0, len(train_accuracies), args.eval_interval))[:len(test_accuracies)]\n",
    "    axes[0,1].plot(eval_epochs, test_accuracies, label='Test', color='red', marker='o')\n",
    "axes[0,1].set_title('Accuracy')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].set_ylabel('Accuracy (%)')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True)\n",
    "\n",
    "# Prototype purity\n",
    "if prototype_purities:\n",
    "    axes[1,0].plot(eval_epochs[:len(prototype_purities)], prototype_purities, color='green', marker='s')\n",
    "    axes[1,0].set_title('Prototype Purity')\n",
    "    axes[1,0].set_xlabel('Epoch')\n",
    "    axes[1,0].set_ylabel('Mean Purity')\n",
    "    axes[1,0].grid(True)\n",
    "\n",
    "# Learning rate\n",
    "axes[1,1].plot(learning_rates, color='purple')\n",
    "axes[1,1].set_title('Learning Rate')\n",
    "axes[1,1].set_xlabel('Epoch')\n",
    "axes[1,1].set_ylabel('Learning Rate')\n",
    "axes[1,1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final metrics:\")\n",
    "print(f\"  Training accuracy: {train_accuracies[-1]:.2f}%\")\n",
    "if test_accuracies:\n",
    "    print(f\"  Test accuracy: {test_accuracies[-1]:.2f}%\")\n",
    "if prototype_purities:\n",
    "    print(f\"  Prototype purity: {prototype_purities[-1]:.3f}\")\n",
    "print(f\"  Best test accuracy: {best_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze_scoring_sheet"
   },
   "outputs": [],
   "source": [
    "# Analyze the learned scoring sheet\n",
    "print(\"Analyzing Learned Scoring Sheet\")\n",
    "print(\"==============================\")\n",
    "\n",
    "model.eval()\n",
    "classifier_weights = model.get_prototype_class_relevance()  # Shape: (prototypes, classes)\n",
    "\n",
    "print(f\"Classifier weight matrix: {classifier_weights.shape}\")\n",
    "print(f\"Weight statistics:\")\n",
    "print(f\"  Mean: {classifier_weights.mean().item():.4f}\")\n",
    "print(f\"  Std: {classifier_weights.std().item():.4f}\")\n",
    "print(f\"  Min: {classifier_weights.min().item():.4f}\")\n",
    "print(f\"  Max: {classifier_weights.max().item():.4f}\")\n",
    "\n",
    "# Find most important prototypes for each class\n",
    "print(f\"\\nTop 3 prototypes for each class:\")\n",
    "for class_idx, class_name in enumerate(class_names):\n",
    "    class_weights = classifier_weights[:, class_idx]\n",
    "    top_prototypes = torch.topk(class_weights, k=3)\n",
    "    print(f\"  {class_name}:\")\n",
    "    for i, (weight, proto_idx) in enumerate(zip(top_prototypes.values, top_prototypes.indices)):\n",
    "        print(f\"    {i+1}. Prototype {proto_idx.item()}: {weight.item():.4f}\")\n",
    "\n",
    "# Visualize scoring sheet as heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "weights_np = classifier_weights.detach().cpu().numpy()\n",
    "\n",
    "# Show only top prototypes for clarity\n",
    "n_top_prototypes = min(50, model.num_prototypes)\n",
    "prototype_importance = weights_np.max(axis=1)  # Max weight across all classes\n",
    "top_prototype_indices = np.argsort(prototype_importance)[-n_top_prototypes:]\n",
    "\n",
    "plt.imshow(weights_np[top_prototype_indices].T, aspect='auto', cmap='viridis')\n",
    "plt.colorbar(label='Relevance Weight')\n",
    "plt.xlabel('Top Prototypes')\n",
    "plt.ylabel('Classes')\n",
    "plt.title(f'Scoring Sheet: Prototype-Class Relevance Weights (Top {n_top_prototypes} Prototypes)')\n",
    "plt.yticks(range(num_classes), class_names)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prototype usage statistics\n",
    "active_prototypes = (weights_np.max(axis=1) > 0.1).sum()\n",
    "print(f\"\\nPrototype usage:\")\n",
    "print(f\"  Active prototypes (weight > 0.1): {active_prototypes}/{model.num_prototypes}\")\n",
    "print(f\"  Utilization rate: {100 * active_prototypes / model.num_prototypes:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example_predictions"
   },
   "outputs": [],
   "source": [
    "# Show example predictions with explanations\n",
    "print(\"Example Predictions with Scoring-Sheet Explanations\")\n",
    "print(\"==================================================\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a batch of test samples\n",
    "    test_inputs, test_targets = next(iter(test_loader))\n",
    "    test_inputs, test_targets = test_inputs.to(args.device), test_targets.to(args.device)\n",
    "    \n",
    "    # Get predictions and explanations\n",
    "    proto_features, pooled_features, logits, class_scores = model(test_inputs, inference=True)\n",
    "    \n",
    "    # Show explanations for first few samples\n",
    "    n_examples = min(3, test_inputs.size(0))\n",
    "    \n",
    "    for i in range(n_examples):\n",
    "        sample_input = test_inputs[i:i+1]\n",
    "        true_class = test_targets[i].item()\n",
    "        \n",
    "        # Get explanation\n",
    "        explanation = model.get_scoring_sheet_explanation(sample_input)\n",
    "        \n",
    "        predicted_class = explanation['predicted_class'][0].item()\n",
    "        class_scores_sample = explanation['class_scores'][0]\n",
    "        contributions = explanation['contributions'][0]\n",
    "        presences = explanation['prototype_presences'][0]\n",
    "        \n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"  True class: {class_names[true_class]}\")\n",
    "        print(f\"  Predicted class: {class_names[predicted_class]}\")\n",
    "        print(f\"  Confidence: {class_scores_sample[predicted_class]:.3f}\")\n",
    "        print(f\"  Correct: {'✓' if predicted_class == true_class else '✗'}\")\n",
    "        \n",
    "        # Show top contributing prototypes\n",
    "        top_contributions = torch.topk(contributions, k=5)\n",
    "        print(f\"  Top contributing prototypes:\")\n",
    "        for j, (contrib, proto_idx) in enumerate(zip(top_contributions.values, top_contributions.indices)):\n",
    "            proto_presence = presences[proto_idx].item()\n",
    "            class_weight = classifier_weights[proto_idx, predicted_class].item()\n",
    "            print(f\"    {j+1}. P{proto_idx.item()}: presence={proto_presence:.3f} × weight={class_weight:.3f} = {contrib.item():.3f}\")\n",
    "        \n",
    "        # Show class scores breakdown\n",
    "        print(f\"  Class scores:\")\n",
    "        sorted_scores, sorted_indices = torch.sort(class_scores_sample, descending=True)\n",
    "        for j in range(min(3, num_classes)):\n",
    "            class_idx = sorted_indices[j].item()\n",
    "            score = sorted_scores[j].item()\n",
    "            print(f\"    {class_names[class_idx]}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_results_section"
   },
   "source": [
    "## 9. Save Results and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_evaluation"
   },
   "outputs": [],
   "source": [
    "# Run final comprehensive evaluation\n",
    "print(\"Running final evaluation...\")\n",
    "\n",
    "final_eval_metrics = evaluate_model(model, test_loader, args.device, num_classes)\n",
    "final_purity_metrics = compute_prototype_purity(model, test_loader, args.device, num_classes)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"FINAL RESULTS\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"Test Accuracy: {final_eval_metrics['accuracy']:.2f}%\")\n",
    "print(f\"Per-class accuracies:\")\n",
    "for i, (class_name, acc) in enumerate(zip(class_names, final_eval_metrics['class_accuracies'])):\n",
    "    print(f\"  {class_name}: {acc:.1f}%\")\n",
    "\n",
    "print(f\"\\nPrototype Analysis:\")\n",
    "print(f\"  Mean prototype purity: {final_purity_metrics['mean_purity']:.3f}\")\n",
    "print(f\"  High purity prototypes (>0.5): {final_purity_metrics['high_purity_prototypes']}\")\n",
    "print(f\"  Active prototypes per sample: {final_eval_metrics['active_prototypes_per_sample']:.1f}\")\n",
    "print(f\"  Unused prototypes: {final_eval_metrics['num_unused_prototypes']}\")\n",
    "print(f\"  Mean prototype activation: {final_eval_metrics['mean_prototype_activation']:.3f}\")\n",
    "\n",
    "print(f\"\\nModel Interpretability:\")\n",
    "active_prototypes = (classifier_weights.max(dim=1)[0] > 0.1).sum().item()\n",
    "print(f\"  Scoring sheet utilization: {100 * active_prototypes / model.num_prototypes:.1f}%\")\n",
    "print(f\"  Average prototypes per class: {classifier_weights.gt(0.1).sum().item() / num_classes:.1f}\")\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Training epochs: {args.epochs}\")\n",
    "print(f\"  Best test accuracy: {best_accuracy:.2f}%\")\n",
    "print(f\"  Final training accuracy: {train_accuracies[-1]:.2f}%\")\n",
    "print(f\"  Prototypes frozen: {args.freeze_prototypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "final_checkpoint = {\n",
    "    'epoch': args.epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'best_accuracy': best_accuracy,\n",
    "    'final_eval_metrics': final_eval_metrics,\n",
    "    'final_purity_metrics': final_purity_metrics,\n",
    "    'training_args': vars(args),\n",
    "    'class_names': class_names,\n",
    "    'num_classes': num_classes,\n",
    "    'num_prototypes': model.num_prototypes\n",
    "}\n",
    "\n",
    "model_path = os.path.join(args.save_dir, 'finetuned_model_final.pth')\n",
    "torch.save(final_checkpoint, model_path)\n",
    "print(f\"Final model saved to: {model_path}\")\n",
    "\n",
    "# Save training history and results\n",
    "results = {\n",
    "    'train_losses': train_losses,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'test_accuracies': test_accuracies,\n",
    "    'prototype_purities': prototype_purities,\n",
    "    'learning_rates': learning_rates,\n",
    "    'best_accuracy': best_accuracy,\n",
    "    'final_accuracy': final_eval_metrics['accuracy'],\n",
    "    'final_purity': final_purity_metrics['mean_purity'],\n",
    "    'training_config': vars(args),\n",
    "    'class_names': class_names\n",
    "}\n",
    "\n",
    "with open(os.path.join(args.save_dir, 'training_results.json'), 'w') as f:\n",
    "    # Convert numpy arrays to lists\n",
    "    results_serializable = {}\n",
    "    for k, v in results.items():\n",
    "        if hasattr(v, 'tolist'):\n",
    "            results_serializable[k] = v.tolist()\n",
    "        else:\n",
    "            results_serializable[k] = v\n",
    "    json.dump(results_serializable, f, indent=2)\n",
    "\n",
    "print(\"Training results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_download_package"
   },
   "outputs": [],
   "source": [
    "# Create downloadable package\n",
    "import zipfile\n",
    "\n",
    "zip_path = 'bcos_pipnet_finetuned_results.zip'\n",
    "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "    # Add model and results\n",
    "    for file in os.listdir(args.save_dir):\n",
    "        if file.endswith(('.pth', '.json')):\n",
    "            zipf.write(os.path.join(args.save_dir, file), f'finetuned/{file}')\n",
    "    \n",
    "    # Add this notebook\n",
    "    if os.path.exists('BcosPIPNet_FineTuning.ipynb'):\n",
    "        zipf.write('BcosPIPNet_FineTuning.ipynb', 'BcosPIPNet_FineTuning.ipynb')\n",
    "\n",
    "print(f\"Results packaged in: {zip_path}\")\n",
    "print(f\"Download this file to save your fine-tuning results!\")\n",
    "\n",
    "# Show file sizes\n",
    "if os.path.exists(model_path):\n",
    "    size_mb = os.path.getsize(model_path) / (1024*1024)\n",
    "    print(f\"Fine-tuned model size: {size_mb:.1f} MB\")\n",
    "\n",
    "if os.path.exists(zip_path):\n",
    "    zip_size_mb = os.path.getsize(zip_path) / (1024*1024)\n",
    "    print(f\"Results archive size: {zip_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "### What we accomplished:\n",
    "\n",
    "1. **✅ Scoring-Sheet Classification**: Implemented linear classifier where weights represent prototype-class relevance\n",
    "2. **✅ Standard NLL Loss**: Used negative log-likelihood with softmax for classification training\n",
    "3. **✅ Interpretable Weights**: Learned weights show which prototypes are relevant to each class\n",
    "4. **✅ Weighted Sum Computation**: Class scores = Σ(prototype_presence × class_weight)\n",
    "5. **✅ Softmax Activation**: Applied during training to convert logits to class probabilities\n",
    "6. **✅ Purity Evaluation**: Implemented prototype purity metrics inspired by PIP-Net\n",
    "\n",
    "### Key Results:\n",
    "- The model learns interpretable prototype-class relationships through the scoring sheet\n",
    "- Each prediction can be explained as a weighted combination of prototype activations\n",
    "- Prototype purity measures show how consistently prototypes activate for specific classes\n",
    "- The approach maintains both accuracy and interpretability\n",
    "\n",
    "### How to use the fine-tuned model:\n",
    "\n",
    "```python\n",
    "# Load the fine-tuned model\n",
    "checkpoint = torch.load('finetuned_model_final.pth')\n",
    "model = create_scoring_sheet_classifier(\n",
    "    pretrained_path='path_to_pretrained.pth',\n",
    "    num_classes=checkpoint['num_classes']\n",
    ")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Get predictions with explanations\n",
    "explanation = model.get_scoring_sheet_explanation(input_image)\n",
    "print(f\"Predicted class: {explanation['predicted_class']}\")\n",
    "print(f\"Top contributing prototypes: {explanation['contributions']}\")\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "1. **Prototype Visualization**: Create visualizations of what each prototype represents\n",
    "2. **Attention Maps**: Generate heatmaps showing where prototypes activate in images\n",
    "3. **Cross-Dataset Transfer**: Test the model on other datasets\n",
    "4. **Human Evaluation**: Study how interpretable the prototype explanations are to humans\n",
    "\n",
    "The fine-tuned model now provides both accurate predictions and interpretable explanations through the scoring-sheet mechanism!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
